<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>Decision Stacks</title>
	<meta property="og:image" content="traj.gif"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Decision Stacks: Flexible Reinforcement Learning
via Modular Generative Models" />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px;margin-bottom: 40px;"><b>Decision Stacks: Flexible Reinforcement Learning
via Modular Generative Models<b></span>
		<table align=center width=600px>
			<table align=center width=600px style="margin-top: 40px;margin-bottom: 20px;">
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href=="https://www.linkedin.com/in/siyanzhao">Siyan Zhao</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://aditya-grover.github.io/">Aditya Grover</a></span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href=''>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/siyanz99/ds'>[Code]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>



	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				Reinforcement learning presents an attractive paradigm to reason about several distinct aspects of sequential decision making, such as specifying complex goals, planning future observations and actions, and critiquing their utilities.
However, the combined integration of these capabilities poses competing algorithmic challenges in retaining maximal expressivity while allowing for flexibility in modeling choices for efficient learning and inference. 
We present Decision Stacks, a generative framework that decomposes goal-conditioned policy agents into 3 generative modules. These modules simulate the temporal evolution of observations, rewards, and actions via independent generative models that can be learned in parallel via teacher forcing. 
 Our framework guarantees both expressivity and flexibility in designing individual modules to account for key factors such as architectural bias, optimization objective and dynamics, transferrability across domains, and inference speed.
  Our empirical results demonstrate the effectiveness of Decision Stacks for offline  policy optimization for several MDP and POMDP environments, outperforming existing methods and enabling flexible generative decision making.
			</td>
		</tr>
	</table>
	<br>

	<hr>
	<center><h1>Decision Stacks</h1></center>
	<center>
		<table align=center width=850px>
			<tr>
				<td width=560px>
					<center>
						<img class="round" style="width:500px" src="./resources/traj.gif"/>
					</center>
				</td>
			</tr>
		</table>
	</center>
<center>
		<table align=center width=850px>
			<tr>
				<td width=560px>
					<center>
						<img class="round" style="width:500px" src="./resources/ds.gif"/>
					</center>
				</td>
			</tr>
		</table>
	</center>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					Illustration for the Decision Stacks framework for learning reinforcement learning agents using probabilistic inference. In contrast to a time-induced ordering,  we propose a modular design that seggregates the modeling of observation, rewards, and action sequences. Each module can be flexibly parameterized via any generative model and the modules are chained via an autoregressive dependency graph to  provide high overall expressivity.
				</td>
			</tr>
		</center>
	</table>



	<hr>

	<center><h1>Performance in both POMDP and MDP locomotion tasks </h1></center>

<div style="display: flex; justify-content: center;margin-bottom: 20px;">
    <table align="center" width="420px" style="margin-right: 20px;">
        <tr>
            <td>
                <b>Offline Reinforcement Learning Performance in POMDP. </b>
		     We generate the POMDPs datasets from the D4RL
locomotion datasets. Decision Stacks
(DS), consistently achieves competitive or superior results
compared to the other algorithms, including BC, DT, TT,
and DD. Notably, DS outperforms other methods in most
environments and attains the highest average score of 74.3,
which reflects a 15.7% performance improvement over the
next best-performing approach Diffuser. This highlights the
effectiveness of our approach in handling POMDP tasks
by more <b> expressively modeling the dependencies <b> among
observations, actions, and rewards
            </td>
        </tr>
    </table>
    <table align="center" width="420px">
        <tr>
            <td>
                <img class="round" style="width: 450px" src="./resources/pomdp.png"/>
            </td>
        </tr>
    </table>
</div>
<div style="display: flex; justify-content: center; margin-top: 20px; margin-bottom: 20px;">
    <table align="center" width="420px" style="margin-right: 20px;">
        <tr>
            <td>
                <b>Offline Reinforcement Learning Performance in MDP. </b>
For a fair comparison, we used the same set of hyperparameters that give the best performance for the DD baseline.
Decision Stacks outperforms or is competitive with the other
baselines on 6/9 environments and is among the highest in
terms of aggregate scores. These results suggest that even in
environments where we can make appropriate conditional
independence assumptions using the MDP framework, the
<b> expressivity <b> in the various modules of Decision Stacks is
helpful for test-time generalization.

            </td>
        </tr>
    </table>
    <table align="center" width="420px">
        <tr>
            <td>
                <img class="round" style="width: 450px" src="./resources/mdp.png"/>
            </td>
        </tr>
    </table>
</div>
<hr>

	<center><h1> Long-Horizon Goal-Conditioned Environments </h1></center>
<div style="display: flex; justify-content: center;margin-bottom: 20px;">
    <table align="center" width="420px" style="margin-top: 20px;">
        <tr>
            <td> We test for the planning capabilities of Decision Stacks
on the Maze2D task from the D4RL bench-
mark. This is a challenging environment requiring an agent
to generate a plan from a start location to a goal location.
The demonstrations contain a sparse reward signal of +1 only when the agent reaches close to the goal.  
 Our experiments demonstrate that Decision Stacks generates robust trajectory plans and matching action sequences, 
outperforming baselines with significant improvements through <b> enhanced modularity <b> and <b> flexible modelling <b>.
	
            </td>
        </tr>
    </table>
    <table align="center" width="420px">
        <tr>
            <td>
               <td><img class="round" style="width:450px" src="./resources/maze.png"/></td>
            </td>
        </tr>
    </table>
</div>	      
		
		<table align=center width=750px>
		<center>
			<tr>
				<td> 
					<td><img class="round" style="width:850px" src="./resources/mazevisual.png"/></td>
				</td>
			</tr>
		</center>
	</table>
<table align=center width=850px style="margin-top: 20px;">
		<center>
			<tr>
				<td> 
					Example rollouts on the Maze2D-medium-v1 environment, where the start positions are consistent across each column, and the
goal position is located at the bottom right corner of the maze. The trajectory waypoints are color-coded, transitioning from blue to red as
time advances. The bottom two rows demonstrate that Diffuser, DD, and DS can generate good plans that can be executed well with a
handcoded controller. However, the respective action models result in differing executions. Compared to DD and Diffuser, DS generates
more flexible and reliable trajectories that align closely with future waypoints planned by the observation model towards the goal.

				</td>
			</tr>
		</center>
	</table>
<hr>

	<center><h1> Architectural Flexibility of Decision Stacks </h1></center>
    </table>
<table align=center width=850px style="margin-top: 20px;">
        <tr>
            <td>
                
		    Decision Stacks distinctly separates the prediction of obser-
vations, rewards, and actions employing three distinct mod-
els that can be trained independently using teacher forcing.
We explore the additional flexibility offered
by different architecture choices for each module. We display a combination of 2x3x3 policy
agents for the Hopper-medium v2 POMDP environment.
Since we adopt a modular structure, we can compose the
different modules efficiently and hence, we only needed
to train 2 (state) + 3 (reward) + 3 (action) models. 
            </td>
        </tr>
    </table>

<table align="center" width="420px" style="margin-top: 20px;">
        <tr>
            <td>
                <img class="round" style="width: 450px" src="./resources/flexibilitytable.png"/>
            </td>
        </tr>
    </table>

	<br>
	<hr>
	<table align=center width=450px>
		<center><h1>Paper and Bibtex</h1></center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="./resources/paperds.png"/></a></td>
			<td><span style="font-size:14pt">Siyan Zhao and Aditya Grover.<br>
				<b>Decision Stacks: Flexible Reinforcement Learning via Modular Generative Models</b><br>
				
				</span>
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>

	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This website template is from <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

